---
id: 9kmoth3bgqdz1qr
slug: llm
---

# LLM

**Description**: Generate text by LLM

**Type**: function

## Inputs

| Name | Type | Description |
| --- | --- | --- |
| **Conversation Id** | text | Id of the conversation. If conversation id is empty, it will be treated as a single-turn conversation, and the context functionality will not be provided |
| **Question** | text | Question text |
## Outputs

| Name | Type | Description |
| --- | --- | --- |
| **Answer** | text | Answer text, will be empty when outputAsEvent is enabled |
## Options

| Name | Type | Description |
| --- | --- | --- |
| **System Context** | text | The global instructions set for the LLM, that will remain continuously in effect. |
| **Model** | text | Model |
| **Temperature** | number | Temperature |
| **Event Output** | boolean | If enabled, the received content will be organized based on punctuation and added to the event queue, thereby achieving rapid feedback. Otherwise, it will wait for the LLM to complete its output and return it as a single string. |
| **Event Queue** | text | Variable name of the event queue. The LLM event format is:   
&#123;  
"name":"llmEvent",  
"timestamp": number,  
"parameters"&#123;  
"text":"Text content of the LLM response",  
"isEnd": boolean  
&#125;  
&#125;.  
The timestamp is the time when the request was initiated. Therefore, events generated within a single request will have the same timestamp. |
| **NodeLogic** | any | Node logic |
